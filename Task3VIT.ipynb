{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets torch torchvision scikit-learn pillow\n\nimport torch\nfrom transformers import (\n    ViTForImageClassification, \n    ViTImageProcessor,\n    Trainer, \n    TrainingArguments\n)\nimport numpy as np\nimport os\nimport warnings\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset as TorchDataset\nfrom torchvision import transforms\n\nwarnings.filterwarnings('ignore')\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"SETUP COMPLETE\")\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\nos.makedirs('./models', exist_ok=True)\nos.makedirs('./outputs', exist_ok=True)\nos.makedirs('./logs', exist_ok=True)\n\nprint(\"All libraries imported!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:54.777753Z","iopub.execute_input":"2025-11-03T17:31:54.778434Z","iopub.status.idle":"2025-11-03T17:31:58.344340Z","shell.execute_reply.started":"2025-11-03T17:31:54.778412Z","shell.execute_reply":"2025-11-03T17:31:58.343170Z"}},"outputs":[{"name":"stdout","text":"SETUP COMPLETE\nDevice: cuda\nGPU: Tesla T4\nMemory: 15.83 GB\nAll libraries imported!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"metrics_store = {}\n\nprint(\"Loading Food-41...\")\n\ndata_path = '/kaggle/input/food41/images'\n\nprint(f\"Dataset path: {data_path}\")\n\nimage_paths = []\nlabels = []\n\nclass_folders = sorted([f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))])\n\nprint(f\"Total classes: {len(class_folders)}\")\nprint(f\"First 5 classes: {class_folders[:5]}\")\n\nlabel_to_idx = {label: idx for idx, label in enumerate(class_folders)}\nidx_to_label = {idx: label for label, idx in label_to_idx.items()}\n\nfor label_name in class_folders:\n    class_path = os.path.join(data_path, label_name)\n    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    for img_name in image_files:\n        image_paths.append(os.path.join(class_path, img_name))\n        labels.append(label_to_idx[label_name])\n    \n    # Show progress for first few classes\n    if len(class_folders) <= 10 or label_name in class_folders[:3]:\n        print(f\"  {label_name}: {len(image_files)} images\")\n\nprint(f\"\\nâœ“ Total images collected: {len(image_paths):,}\")\n\nSAMPLE_RATIO = 0.6\nsample_size = int(len(image_paths) * SAMPLE_RATIO)\nindices = np.random.choice(len(image_paths), sample_size, replace=False)\n\nimage_paths = [image_paths[i] for i in indices]\nlabels = [labels[i] for i in indices]\n\nprint(f\"Using {len(image_paths):,} images ({SAMPLE_RATIO*100:.0f}% sample)\")\n\n# Split dataset\ntrain_paths, temp_paths, train_labels, temp_labels = train_test_split(\n    image_paths, labels, test_size=0.3, random_state=42, stratify=labels\n)\n\nval_paths, test_paths, val_labels, test_labels = train_test_split(\n    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n)\n\nprint(f\"Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n\nmetrics_store['vit'] = {\n    'label_to_idx': label_to_idx,\n    'idx_to_label': idx_to_label,\n    'num_classes': len(class_folders)\n}\n\nprint(\"Dataset loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:58.346004Z","iopub.execute_input":"2025-11-03T17:31:58.346266Z","iopub.status.idle":"2025-11-03T17:31:58.922468Z","shell.execute_reply.started":"2025-11-03T17:31:58.346245Z","shell.execute_reply":"2025-11-03T17:31:58.921539Z"}},"outputs":[{"name":"stdout","text":"Loading Food-41...\nDataset path: /kaggle/input/food41/images\nTotal classes: 101\nFirst 5 classes: ['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']\n  apple_pie: 1000 images\n  baby_back_ribs: 1000 images\n  baklava: 1000 images\n\nâœ“ Total images collected: 101,000\nUsing 60,600 images (60% sample)\nTrain: 42,420 | Val: 9,090 | Test: 9,090\nDataset loaded\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class FoodDataset(TorchDataset):\n    def __init__(self, image_paths, labels, processor, augment=False):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.processor = processor\n        self.augment = augment\n        \n        if augment:\n            self.transform = transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(0.2, 0.2, 0.2),\n                transforms.RandomRotation(10),\n            ])\n        else:\n            self.transform = transforms.Resize((224, 224))\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        try:\n            image = Image.open(self.image_paths[idx]).convert('RGB')\n        except:\n            image = Image.new('RGB', (224, 224), 'white')\n        \n        image = self.transform(image)\n        encoding = self.processor(image, return_tensors='pt')\n        \n        return {\n            'pixel_values': encoding['pixel_values'].squeeze(),\n            'labels': torch.tensor(self.labels[idx])\n        }\n\nprint(\"ViT...\")\n\nvit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nvit_model = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224',\n    num_labels=len(class_folders),\n    ignore_mismatched_sizes=True\n)\n\nvit_model.config.id2label = idx_to_label\nvit_model.config.label2id = label_to_idx\n\nprint(f\"âœ“ ViT loaded ({vit_model.num_parameters() / 1e6:.1f}M params)\")\n\n# Create datasets\nvit_train_dataset = FoodDataset(train_paths, train_labels, vit_processor, augment=True)\nvit_val_dataset = FoodDataset(val_paths, val_labels, vit_processor, augment=False)\nvit_test_dataset = FoodDataset(test_paths, test_labels, vit_processor, augment=False)\n\nprint(f\"Datasets created\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return {'accuracy': accuracy_score(labels, predictions)}\n\nvit_training_args = TrainingArguments(\n    output_dir='./models/vit-food',\n    num_train_epochs=3,                    \n    per_device_train_batch_size=32,       \n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=1,       \n    learning_rate=2e-4,\n    warmup_steps=200,\n    weight_decay=0.01,\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=1,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    remove_unused_columns=False,\n    report_to=\"none\",\n)\n\nprint(f\"Epochs: {vit_training_args.num_train_epochs}\")\nprint(f\"Batch size: {vit_training_args.per_device_train_batch_size}\")\n\nvit_trainer = Trainer(\n    model=vit_model,\n    args=vit_training_args,\n    train_dataset=vit_train_dataset,\n    eval_dataset=vit_val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"\\nðŸš€ Training started...\\n\")\ntrain_result = vit_trainer.train()\n\nprint(\"\\nâœ“ Training complete!\")\nprint(f\"Training time: {train_result.metrics['train_runtime']:.2f}s\")\n\n# Save\nvit_trainer.save_model('./models/vit-food-final')\nvit_processor.save_pretrained('./models/vit-food-final')\n\n# Store metrics\nmetrics_store['vit']['train_loss'] = train_result.metrics['train_loss']\nmetrics_store['vit']['train_time'] = train_result.metrics['train_runtime']\n\nprint(\"Model saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:59.430829Z","iopub.status.idle":"2025-11-03T17:31:59.431195Z","shell.execute_reply.started":"2025-11-03T17:31:59.430994Z","shell.execute_reply":"2025-11-03T17:31:59.431009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Path to your trained model folder\nmodel_folder = \"./models/vit-food-final\"\nzip_filename = \"vit_food_final.zip\"\n\n# Make sure the folder exists\nif os.path.exists(model_folder):\n    # Create ZIP archive\n    shutil.make_archive(zip_filename.replace(\".zip\", \"\"), 'zip', model_folder)\n    print(f\"Zipped model saved as {zip_filename}\")\nelse:\n    print(\"Model folder not found! Check the path and try again.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:59.432060Z","iopub.status.idle":"2025-11-03T17:31:59.432424Z","shell.execute_reply.started":"2025-11-03T17:31:59.432258Z","shell.execute_reply":"2025-11-03T17:31:59.432273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'vit_food_final.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:59.433910Z","iopub.status.idle":"2025-11-03T17:31:59.434258Z","shell.execute_reply.started":"2025-11-03T17:31:59.434056Z","shell.execute_reply":"2025-11-03T17:31:59.434072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio gdown \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:59.435128Z","iopub.status.idle":"2025-11-03T17:31:59.435409Z","shell.execute_reply.started":"2025-11-03T17:31:59.435281Z","shell.execute_reply":"2025-11-03T17:31:59.435294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(list(vit_model.config.id2label.keys())[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:32:03.717009Z","iopub.execute_input":"2025-11-03T17:32:03.717375Z","iopub.status.idle":"2025-11-03T17:32:03.721752Z","shell.execute_reply.started":"2025-11-03T17:32:03.717352Z","shell.execute_reply":"2025-11-03T17:32:03.720891Z"}},"outputs":[{"name":"stdout","text":"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import gradio as gr\nfrom PIL import Image\nimport torch\nimport gdown\nimport zipfile\nimport os\nfrom transformers import ViTImageProcessor, ViTForImageClassification\n\n\n# Download fine-tuned model\n\ndrive_url = \"https://drive.google.com/uc?id=11sQr1xMlFhgQmE6eLaEH-yTLKfTReBZn\"\nzip_path = \"vit_food_final.zip\"\nmodel_dir = \"./vit_food_final\"\n\nif not os.path.exists(model_dir):\n    print(\"Downloading fine-tuned model from Google Drive...\")\n    gdown.download(drive_url, zip_path, quiet=False)\n\n    print(\"Extracting model...\")\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(model_dir)\n    print(\"Extracted to:\", model_dir)\nelse:\n    print(\"Model folder already exists, skipping download.\")\n\n# Load model and processor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nvit_processor = ViTImageProcessor.from_pretrained(model_dir)\nvit_model = ViTForImageClassification.from_pretrained(model_dir).to(device)\n\n# Automatically pulled from config.json\nid2label = vit_model.config.id2label\nlabel2id = vit_model.config.label2id\n\nprint(f\"âœ“ Loaded ViT model with {len(id2label)} labels\")\n\n\n# Prediction Function\ndef predict_food(image):\n    \"\"\"Predict food class from uploaded image\"\"\"\n    try:\n        # Prepare image\n        if not isinstance(image, Image.Image):\n            image = Image.fromarray(image)\n        image = image.convert('RGB').resize((224, 224))\n        \n        # Prepare inputs\n        inputs = vit_processor(image, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        vit_model.to(device)\n        vit_model.eval()\n\n        with torch.no_grad():\n            outputs = vit_model(**inputs)\n            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n        \n        # Get top 5 predictions\n        top5_prob, top5_idx = torch.topk(probs, 5)\n        results = {}\n\n        for prob, idx in zip(top5_prob.cpu().numpy(), top5_idx.cpu().numpy()):\n            # Convert index to str key if needed\n            idx_str = str(int(idx))\n            # Ensure we handle both string/int keys in id2label safely\n            label_key = id2label.get(idx_str, id2label.get(int(idx), f\"Unknown-{idx}\"))\n            \n            # Clean and format label\n            class_name = str(label_key).replace('_', ' ').title()\n            results[class_name] = float(round(prob, 4))  # ensure pure float\n\n        print(\"Predictions:\", results)\n        return results\n\n    except Exception as e:\n        print(\"Error:\", e)\n        return {\"Error\": str(e)}\n\n# Gradio Interface\n\ndemo = gr.Interface(\n    fn=predict_food,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Food Image\"),\n    outputs=gr.Label(num_top_classes=5, label=\"Top 5 Predictions\"),\n    title=\"Food-101 Classifier (Fine-Tuned ViT)\",\n    description=\"\"\"\n    Upload a food image to classify it into one of 101 food categories.  \n    **Model:** Fine-tuned ViT (Base Patch-16 224)  \n    **Dataset:** Food-101 | Trained using limited Kaggle GPU resources.\n    \"\"\",\n    # examples=examples,\n    theme=gr.themes.Soft()\n)\n\nprint(\"Gradio interface ready\")\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T18:12:01.590654Z","iopub.execute_input":"2025-11-03T18:12:01.591442Z","iopub.status.idle":"2025-11-03T18:18:54.097477Z","shell.execute_reply.started":"2025-11-03T18:12:01.591416Z","shell.execute_reply":"2025-11-03T18:18:54.096774Z"}},"outputs":[{"name":"stdout","text":"Model folder already exists, skipping download.\nâœ“ Loaded ViT model with 101 labels\nGradio interface ready\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://f638edbbb276b5d011.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f638edbbb276b5d011.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Predictions: {'Hamburger': 0.9922999739646912, 'Grilled Cheese Sandwich': 0.0010999999940395355, 'Pulled Pork Sandwich': 0.0008999999845400453, 'French Fries': 0.0005000000237487257, 'Falafel': 0.00039999998989515007}\nKeyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://f638edbbb276b5d011.gradio.live\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# # Cell: Gradio Interface\n# import gradio as gr\n# from PIL import Image\n# import torch\n\n# def predict_food(image):\n#     \"\"\"Predict food class from uploaded image\"\"\"\n#     try:\n#         # Prepare image\n#         if not isinstance(image, Image.Image):\n#             image = Image.fromarray(image)\n#         image = image.convert('RGB').resize((224, 224))\n        \n#         # Get predictions\n#         inputs = vit_processor(image, return_tensors=\"pt\")\n#         if torch.cuda.is_available():\n#             inputs = {k: v.to(device) for k, v in inputs.items()}\n#             vit_model.to(device)\n        \n#         vit_model.eval()\n#         with torch.no_grad():\n#             outputs = vit_model(**inputs)\n#             probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n        \n#         # Get top 5 predictions\n#         top5_prob, top5_idx = torch.topk(probs, 5)\n#         results = {}\n#         for prob, idx in zip(top5_prob.cpu().numpy(), top5_idx.cpu().numpy()):\n#             class_name = idx_to_label[int(idx)].replace('_', ' ').title()\n#             results[class_name] = float(prob)\n        \n#         return results\n#     except Exception as e:\n#         return {\"Error\": str(e)}\n\n# # Prepare examples\n# examples = test_paths[:5] if len(test_paths) >= 5 else []\n\n# # Create interface\n# demo = gr.Interface(\n#     fn=predict_food,\n#     inputs=gr.Image(type=\"pil\", label=\"Upload Food Image\"),\n#     outputs=gr.Label(num_top_classes=5, label=\"Top 5 Predictions\"),\n#     title=\"Food-41 Classifier\",\n#     description=f\"\"\"\n#     Upload a food image to classify it into one of {len(class_folders)} categories.\n    \n#     **Model:** Vision Transformer (ViT) | **Parameters:** 85.9M\n#     **Dataset:** {len(train_paths):,} training images | {len(val_paths):,} validation | {len(test_paths):,} test\n#     \"\"\",\n#     examples=examples if examples else None,\n#     theme=gr.themes.Soft()\n# )\n\n# print(\"GRADIO INTERFACE READY\")\n# demo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T17:31:59.438886Z","iopub.status.idle":"2025-11-03T17:31:59.439139Z","shell.execute_reply.started":"2025-11-03T17:31:59.439008Z","shell.execute_reply":"2025-11-03T17:31:59.439018Z"}},"outputs":[],"execution_count":null}]}